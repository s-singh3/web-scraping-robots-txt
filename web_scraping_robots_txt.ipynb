{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "# List of websites to scrape\n",
        "websites = [\n",
        "    \"https://www.google.com/\",\n",
        "    \"https://www.facebook.com/\",\n",
        "    \"https://www.amazon.com/\",\n",
        "    \"https://www.twitter.com/\",\n",
        "    \"https://www.wikipedia.org/\",\n",
        "    \"https://www.reddit.com/\",\n",
        "    \"https://www.nytimes.com/\",\n",
        "    \"https://www.bbc.com/\",\n",
        "]"
      ],
      "metadata": {
        "id": "83aXYXWxJHlB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_robots_txt(url):\n",
        "    \"\"\"Fetches the robots.txt file from a given website.\"\"\"\n",
        "    robots_url = urljoin(url, \"/robots.txt\")\n",
        "    try:\n",
        "        response = requests.get(robots_url, timeout=5)\n",
        "        if response.status_code == 200:\n",
        "            return response.text\n",
        "        else:\n",
        "            return None\n",
        "    except requests.RequestException:\n",
        "        return None"
      ],
      "metadata": {
        "id": "ivi5ZAnHJJms"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_robots_txt(robots_txt):\n",
        "    \"\"\"Parses the robots.txt file and extracts crawl rules.\"\"\"\n",
        "    rules = {\n",
        "        \"Allow\": [],\n",
        "        \"Disallow\": []\n",
        "    }\n",
        "    if robots_txt:\n",
        "        for line in robots_txt.split(\"\\n\"):\n",
        "            parts = line.split(\": \", 1)  # Safely split into two parts\n",
        "            if len(parts) == 2:  # Ensure there are both key and value\n",
        "                key, value = parts\n",
        "                if key == \"Allow\":\n",
        "                    rules[\"Allow\"].append(value)\n",
        "                elif key == \"Disallow\":\n",
        "                    rules[\"Disallow\"].append(value)\n",
        "    return rules\n"
      ],
      "metadata": {
        "id": "0kNMMWsaJNhh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpxY7QJzJGCy",
        "outputId": "18a70970-2011-409a-d00f-12cfcd25e246"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                      Website  Allowed Paths  Disallowed Paths  \\\n",
            "0     https://www.google.com/             89               248   \n",
            "1   https://www.facebook.com/             84               686   \n",
            "2     https://www.amazon.com/             17               121   \n",
            "3    https://www.twitter.com/              4                20   \n",
            "4  https://www.wikipedia.org/              3               459   \n",
            "5     https://www.reddit.com/              0                 1   \n",
            "6    https://www.nytimes.com/             19               124   \n",
            "7        https://www.bbc.com/              0                67   \n",
            "\n",
            "                                  Example Disallowed  \n",
            "0        [/search, /sdch, /groups, /index.html?, /?]  \n",
            "1                                    [/, /, /, /, /]  \n",
            "2  [/exec/obidos/account-access-login, /exec/obid...  \n",
            "3  [/search/realtime, /search/users, /search/*/gr...  \n",
            "4                                    [/, /, /, /, /]  \n",
            "5                                                [/]  \n",
            "6  [/ads/, /adx/bin/, /athletic/wp/wp-admin/, /at...  \n",
            "7  [/bitesize/search$, /bitesize/search/, /bitesi...  \n"
          ]
        }
      ],
      "source": [
        "data = []\n",
        "\n",
        "for site in websites:\n",
        "    robots_txt = fetch_robots_txt(site)\n",
        "    rules = parse_robots_txt(robots_txt)\n",
        "    data.append({\n",
        "        \"Website\": site,\n",
        "        \"Allowed Paths\": len(rules[\"Allow\"]),\n",
        "        \"Disallowed Paths\": len(rules[\"Disallow\"]),\n",
        "        \"Example Disallowed\": rules[\"Disallow\"][:5]  # Example of first 5 disallowed paths\n",
        "    })\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df)\n",
        "\n",
        "# Save to CSV\n",
        "df.to_csv(\"robots_txt_analysis.csv\", index=False)\n"
      ]
    }
  ]
}